{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b911c684-a047-4274-918d-1aeedab24466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd95dda0a60e41ab8875a8ce759cf31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15278\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\15278\\.cache\\huggingface\\hub\\datasets--SemEvalWorkshop--sem_eval_2010_task_8. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e323de45c2f441219953336c7557b05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/673k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e407dc981e8345629b1d6270e18decde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/231k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24881e784264fd08dbda801541507d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63f11e8d9184bc88788c9dc491009d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2717 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0894e29-d890-4eb6-9251-3392ead36b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'relation'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'relation'],\n",
       "        num_rows: 2717\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baa772f5-adf0-4d4e-90c4-29ddbd3f1817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': ['The system as described above has its greatest application in an arrayed <e1>configuration</e1> of antenna <e2>elements</e2>.', 'The <e1>child</e1> was carefully wrapped and bound into the <e2>cradle</e2> by means of a cord.', 'The <e1>author</e1> of a keygen uses a <e2>disassembler</e2> to look at the raw assembly code.', 'A misty <e1>ridge</e1> uprises from the <e2>surge</e2>.', 'The <e1>student</e1> <e2>association</e2> is the voice of the undergraduate student population of the State University of New York at Buffalo.'], 'relation': [3, 18, 11, 18, 12]}\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"train\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe07fcf-8bc3-4c62-ae29-b2457ad3535a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b442bc33-2dcf-459c-a82b-2c8707c55a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\15278\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")  # Ensure the nltk tokenizer is available\n",
    "\n",
    "\n",
    "def extract_entities(sentence):\n",
    "    \"\"\"Extract entities within <e1> and <e2> tags and tokenize the sentence.\"\"\"\n",
    "    e1 = re.findall(r'<e1>(.*?)</e1>', sentence)[0]\n",
    "    e2 = re.findall(r'<e2>(.*?)</e2>', sentence)[0]\n",
    "\n",
    "    # Replace entity markers to ensure proper tokenization\n",
    "    sentence = sentence.replace(f'<e1>{e1}</e1>', f' <e1> {e1} </e1> ')\n",
    "    sentence = sentence.replace(f'<e2>{e2}</e2>', f' <e2> {e2} </e2> ')\n",
    "\n",
    "    # Tokenize the sentence\n",
    "    tokens = word_tokenize(sentence)\n",
    "\n",
    "    return \" \".join(tokens)  # Return the processed string\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    \"\"\"Preprocess the dataset by extracting text and labels.\"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    for data in dataset:\n",
    "        sentence = extract_entities(data[\"sentence\"])  # Process the sentence\n",
    "        relation = data[\"relation\"]  # Extract the relation label\n",
    "        sentences.append(sentence)\n",
    "        labels.append(relation)\n",
    "\n",
    "    return sentences, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "418e7b75-fa8a-46ac-a283-59f99f36f9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.6467\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.76      0.80       134\n",
      "           1       0.82      0.87      0.84       194\n",
      "           2       0.60      0.75      0.67       162\n",
      "           3       0.51      0.54      0.52       150\n",
      "           4       0.76      0.86      0.81       153\n",
      "           5       0.83      0.74      0.78        39\n",
      "           6       0.78      0.86      0.82       291\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.72      0.81      0.76       211\n",
      "           9       0.69      0.62      0.65        47\n",
      "          10       0.43      0.45      0.44        22\n",
      "          11       0.58      0.54      0.56       134\n",
      "          12       0.69      0.34      0.46        32\n",
      "          13       0.70      0.91      0.79       201\n",
      "          14       0.65      0.71      0.68       210\n",
      "          15       0.56      0.43      0.49        51\n",
      "          16       0.61      0.54      0.57       108\n",
      "          17       0.54      0.41      0.47       123\n",
      "          18       0.35      0.26      0.30       454\n",
      "\n",
      "    accuracy                           0.65      2717\n",
      "   macro avg       0.61      0.60      0.60      2717\n",
      "weighted avg       0.63      0.65      0.63      2717\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load the SemEval-2010 Task 8 dataset\n",
    "    dataset = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    train_sentences, train_labels = preprocess_dataset(dataset[\"train\"])\n",
    "    test_sentences, test_labels = preprocess_dataset(dataset[\"test\"])\n",
    "\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=4000)  \n",
    "    X_train = vectorizer.fit_transform(train_sentences)\n",
    "    X_test = vectorizer.transform(test_sentences)\n",
    "\n",
    "    svm_model = SVC(kernel=\"linear\", C=5.0, class_weight=\"balanced\", probability=True)\n",
    "    svm_model.fit(X_train, train_labels)\n",
    "\n",
    "    # predictions\n",
    "    predictions = svm_model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    print(f\"Test set accuracy: {accuracy:.4f}\\n\")\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(test_labels, predictions, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a895e6e-4b59-4a4c-88a4-315826aae612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "triu = linalg.triu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d37d4-8fc5-421e-80fd-816006695a28",
   "metadata": {},
   "source": [
    "pip install scipy==1.12\n",
    "#https://stackoverflow.com/questions/78555551/importerror-cannot-import-name-triu-from-scipy-linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2233eb3-03c3-42ed-8882-d7504530d7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f07caa7d-5bc6-48d3-8669-5244586a8188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\15278\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\15278\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.tag import hmm\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "nltk.download(\"punkt\")  # Ensure the NLTK tokenizer is available\n",
    "nltk.download(\"conll2000\")  # Download the conll2000 dataset for HMM training\n",
    "\n",
    "\n",
    "# HMM model class, used as a preprocessing step for SVM input features\n",
    "class HMMFeatureExtractor(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        # Train a simple HMM model using NLTK's provided training data\n",
    "        self.hmm_tagger = self.train_hmm()\n",
    "\n",
    "    def train_hmm(self):\n",
    "        # Train an HMM model using the conll2000 corpus\n",
    "        train_data = conll2000.tagged_sents()\n",
    "        trainer = hmm.HiddenMarkovModelTrainer()\n",
    "        return trainer.train(train_data)  # Train with a limited number of iterations\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Convert each sentence into a sequence of POS tags\"\"\"\n",
    "        transformed_data = []\n",
    "        for sentence in X:\n",
    "            tokens = word_tokenize(sentence)\n",
    "            tagged = self.hmm_tagger.tag(tokens)  # Get POS tags\n",
    "            transformed_data.append([tag for _, tag in tagged])\n",
    "        return transformed_data\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Prediction function can be empty (we only care about feature extraction here)\n",
    "        return np.zeros(len(X))\n",
    "\n",
    "\n",
    "def extract_entities(sentence):\n",
    "    \"\"\"Extract entities from <e1> and <e2> tags and tokenize the sentence\"\"\"\n",
    "    e1 = re.findall(r'<e1>(.*?)</e1>', sentence)[0]\n",
    "    e2 = re.findall(r'<e2>(.*?)</e2>', sentence)[0]\n",
    "\n",
    "    # Replace entity markers to make them recognizable\n",
    "    sentence = sentence.replace(f'<e1>{e1}</e1>', f' <e1> {e1} </e1> ')\n",
    "    sentence = sentence.replace(f'<e2>{e2}</e2>', f' <e2> {e2} </e2> ')\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(sentence)\n",
    "\n",
    "    return \" \".join(tokens)  # Return the processed string\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    \"\"\"Preprocess the dataset by extracting text and labels\"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    for data in dataset:\n",
    "        sentence = extract_entities(data[\"sentence\"])  # Process the sentence\n",
    "        relation = data[\"relation\"]  # Relation label\n",
    "        sentences.append(sentence)\n",
    "        labels.append(relation)\n",
    "\n",
    "    return sentences, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1683c751-e4f4-4056-bf65-aa16dc74b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load the SemEval-2010 Task 8 dataset\n",
    "    dataset = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    train_sentences, train_labels = preprocess_dataset(dataset[\"train\"])\n",
    "    test_sentences, test_labels = preprocess_dataset(dataset[\"test\"])\n",
    "\n",
    "    # TF-IDF vectorization\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=7000)\n",
    "    X_train_tfidf = vectorizer.fit_transform(train_sentences)\n",
    "    X_test_tfidf = vectorizer.transform(test_sentences)\n",
    "\n",
    "    # Extract POS features using HMM\n",
    "    hmm_extractor = HMMFeatureExtractor()\n",
    "    X_train_hmm = hmm_extractor.transform(train_sentences)\n",
    "    X_test_hmm = hmm_extractor.transform(test_sentences)\n",
    "\n",
    "    # Encode POS tags using CountVectorizer\n",
    "    pos_vectorizer = CountVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "    X_train_hmm_encoded = pos_vectorizer.fit_transform(X_train_hmm)\n",
    "    X_test_hmm_encoded = pos_vectorizer.transform(X_test_hmm)\n",
    "\n",
    "    # Combine TF-IDF features and HMM-based POS features\n",
    "    X_train_combined = np.hstack([X_train_tfidf.toarray(), X_train_hmm_encoded.toarray()])\n",
    "    X_test_combined = np.hstack([X_test_tfidf.toarray(), X_test_hmm_encoded.toarray()])\n",
    "\n",
    "    # Train an SVM model\n",
    "    svm_model = SVC(kernel=\"linear\", C=5.0, class_weight=\"balanced\", probability=True)\n",
    "    svm_model.fit(X_train_combined, train_labels)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = svm_model.predict(X_test_combined)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\\n\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(test_labels, predictions, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd95271f-0c21-41d1-8c71-81d22d9728ac",
   "metadata": {},
   "source": [
    "HMMFeatureExtractor 生成的是 词性序列，然后用 CountVectorizer 进行编码。\n",
    "\n",
    "CountVectorizer 的输出是固定维度的 词频矩阵（Bag-of-Words），不需要手动 padding。\n",
    "SVM 直接处理这些固定维度的特征矩阵，不会因为序列长度不同而报错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342428de-57a2-475c-951a-2dfc5b5a087d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c026f2-dc35-4c24-9af0-0f13d76cf6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51ff3e7-24f2-4d0e-9f2e-0e8d0beca7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "from nltk.tag import hmm\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")  # Ensure word tokenizer is available\n",
    "nltk.download(\"conll2000\")  # Download conll2000 dataset for training HMM\n",
    "nltk.download('punkt_tab')  # Download missing punkt_tab resource\n",
    "\n",
    "# HMM model class for feature extraction before feeding data to the classifier\n",
    "class HMMFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.hmm_tagger = self.train_hmm()\n",
    "\n",
    "    def train_hmm(self):\n",
    "        train_data = conll2000.tagged_sents()\n",
    "        trainer = hmm.HiddenMarkovModelTrainer()\n",
    "        return trainer.train(train_data)  # Train an HMM tagger\n",
    "\n",
    "    def transform(self, X):\n",
    "        transformed_data = []\n",
    "        for sentence in X:\n",
    "            tokens = word_tokenize(sentence)\n",
    "            tagged = self.hmm_tagger.tag(tokens)  # Get POS tags\n",
    "            transformed_data.append([tag for _, tag in tagged])\n",
    "        return transformed_data\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.zeros(len(X))\n",
    "\n",
    "\n",
    "# Function to extract entities and tokenize sentences\n",
    "def extract_entities(sentence):\n",
    "    e1 = re.findall(r'<e1>(.*?)</e1>', sentence)[0]\n",
    "    e2 = re.findall(r'<e2>(.*?)</e2>', sentence)[0]\n",
    "\n",
    "    sentence = sentence.replace(f'<e1>{e1}</e1>', f' <e1> {e1} </e1> ')\n",
    "    sentence = sentence.replace(f'<e2>{e2}</e2>', f' <e2> {e2} </e2> ')\n",
    "\n",
    "    tokens = word_tokenize(sentence)\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# Function to preprocess the dataset\n",
    "def preprocess_dataset(dataset):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    for data in dataset:\n",
    "        sentence = extract_entities(data[\"sentence\"])\n",
    "        relation = data[\"relation\"]\n",
    "        sentences.append(sentence)\n",
    "        labels.append(relation)\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the SemEval-2010 Task 8 dataset\n",
    "    dataset = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    train_sentences, train_labels = preprocess_dataset(dataset[\"train\"])\n",
    "    test_sentences, test_labels = preprocess_dataset(dataset[\"test\"])\n",
    "\n",
    "    # TF-IDF vectorization\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=20000)\n",
    "    X_train_tfidf = vectorizer.fit_transform(train_sentences)\n",
    "    X_test_tfidf = vectorizer.transform(test_sentences)\n",
    "\n",
    "    # Extract POS features using HMM\n",
    "    hmm_extractor = HMMFeatureExtractor()\n",
    "    X_train_hmm = hmm_extractor.transform(train_sentences)\n",
    "    X_test_hmm = hmm_extractor.transform(test_sentences)\n",
    "\n",
    "    # Encode POS tags using CountVectorizer\n",
    "    pos_vectorizer = CountVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "    X_train_hmm_encoded = pos_vectorizer.fit_transform(X_train_hmm)\n",
    "    X_test_hmm_encoded = pos_vectorizer.transform(X_test_hmm)\n",
    "\n",
    "    # Combine TF-IDF features and HMM-based POS features\n",
    "    X_train_combined = np.hstack([X_train_tfidf.toarray(), X_train_hmm_encoded.toarray()])\n",
    "    X_test_combined = np.hstack([X_test_tfidf.toarray(), X_test_hmm_encoded.toarray()])\n",
    "\n",
    "    # Convert data to PyTorch tensors and transfer to GPU\n",
    "    X_train_tensor = torch.tensor(X_train_combined, dtype=torch.float32).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test_combined, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Convert labels to numerical format\n",
    "    label_mapping = {label: idx for idx, label in enumerate(set(train_labels))}\n",
    "    y_train = torch.tensor([label_mapping[label] for label in train_labels], dtype=torch.long).to(device)\n",
    "    y_test = torch.tensor([label_mapping[label] for label in test_labels], dtype=torch.long).to(device)\n",
    "\n",
    "    # Create the model\n",
    "    model = SimpleNN(input_dim=X_train_tensor.shape[1], output_dim=len(label_mapping)).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 3000\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test.cpu(), predicted.cpu())\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test.cpu(), predicted.cpu(), target_names=[str(key) for key in label_mapping.keys()], zero_division=0))  # Convert keys to strings\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
