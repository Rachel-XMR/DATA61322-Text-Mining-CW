{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO1wkpIfvwR5QpI2/f0UAkY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8tMd_jkcADUf","executionInfo":{"status":"ok","timestamp":1741256960100,"user_tz":0,"elapsed":15543,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"d20082e8-135c-49ae-f4ff-f3b92e6ac1fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install gradio"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4E1taIFZBJsy","executionInfo":{"status":"ok","timestamp":1741257234848,"user_tz":0,"elapsed":14618,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"ad72c604-8434-44d6-b44a-2f122cf162ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gradio\n","  Downloading gradio-5.20.0-py3-none-any.whl.metadata (16 kB)\n","Collecting aiofiles<24.0,>=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n","Collecting fastapi<1.0,>=0.115.2 (from gradio)\n","  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n","Collecting ffmpy (from gradio)\n","  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n","Collecting gradio-client==1.7.2 (from gradio)\n","  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting groovy~=0.1 (from gradio)\n","  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n","Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n","Collecting markupsafe~=2.0 (from gradio)\n","  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n","Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n","Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n","Collecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting python-multipart>=0.0.18 (from gradio)\n","  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n","Collecting ruff>=0.9.3 (from gradio)\n","  Downloading ruff-0.9.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n","  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n","Collecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting starlette<1.0,>=0.40.0 (from gradio)\n","  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\n","Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n","  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n","Collecting uvicorn>=0.14.0 (from gradio)\n","  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n","Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","Downloading gradio-5.20.0-py3-none-any.whl (62.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n","Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n","Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n","Downloading ruff-0.9.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n","Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading starlette-0.46.0-py3-none-any.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n","Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n","Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n","  Attempting uninstall: markupsafe\n","    Found existing installation: MarkupSafe 3.0.2\n","    Uninstalling MarkupSafe-3.0.2:\n","      Successfully uninstalled MarkupSafe-3.0.2\n","Successfully installed aiofiles-23.2.1 fastapi-0.115.11 ffmpy-0.5.0 gradio-5.20.0 gradio-client-1.7.2 groovy-0.1.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.9 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.0 tomlkit-0.13.2 uvicorn-0.34.0\n"]}]},{"cell_type":"code","source":["%%writefile preprocess.py\n","import os\n","import torch\n","import numpy as np\n","import json\n","import argparse\n","from tqdm import tqdm\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser(description='准备模型部署所需文件')\n","    parser.add_argument('--embedding_path', type=str, required=True, help='GloVe词嵌入文件路径')\n","    parser.add_argument('--relation_path', type=str, required=True, help='关系映射文件路径')\n","    parser.add_argument('--model_path', type=str, required=True, help='训练好的模型权重文件路径')\n","    parser.add_argument('--output_dir', type=str, required=True, help='输出目录')\n","    parser.add_argument('--word_dim', type=int, default=100, help='词嵌入维度')\n","    return parser.parse_args()\n","\n","def prepare_embeddings(embedding_filepath, embedding_dimension, output_dir):\n","    \"\"\"处理并保存词嵌入\"\"\"\n","    print(f\"正在处理词嵌入: {embedding_filepath}\")\n","\n","    word_to_id = {'PAD': 0, 'UNK': 1, '<e1>': 2, '<e2>': 3, '</e1>': 4, '</e2>': 5}\n","    word_vectors = []\n","\n","    # 读取GloVe词嵌入\n","    with open(embedding_filepath, 'r', encoding='UTF-8') as file:\n","        for line in tqdm(file, desc=\"读取词嵌入\"):\n","            elements = line.strip().split()\n","            if len(elements) == embedding_dimension + 1:\n","                word, vector = elements[0], np.array(elements[1:], dtype=np.float32)\n","                word_to_id[word] = len(word_to_id)\n","                word_vectors.append(vector)\n","\n","    # 处理词嵌入\n","    word_vectors = np.stack(word_vectors)\n","    vector_mean, vector_std = word_vectors.mean(), word_vectors.std()\n","    special_vectors = np.random.normal(vector_mean, vector_std, (6, embedding_dimension))\n","    special_vectors[0] = 0  # PAD向量设为0\n","    word_vectors = np.concatenate((special_vectors, word_vectors), axis=0)\n","\n","    # 转换为PyTorch张量并保存\n","    word_embeddings = torch.from_numpy(word_vectors.astype(np.float32))\n","    torch.save(word_embeddings, os.path.join(output_dir, 'word_embeddings.pt'))\n","\n","    # 保存word_to_id映射\n","    with open(os.path.join(output_dir, 'word_to_id.json'), 'w', encoding='utf-8') as f:\n","        json.dump(word_to_id, f, ensure_ascii=False, indent=2)\n","\n","    print(f\"词嵌入已保存，词汇量: {len(word_to_id)}\")\n","    return word_to_id, word_embeddings\n","\n","def prepare_relation_mapping(relation_filepath, output_dir):\n","    \"\"\"处理并保存关系映射\"\"\"\n","    print(f\"正在处理关系映射: {relation_filepath}\")\n","\n","    relation_to_id, id_to_relation = {}, {}\n","\n","    try:\n","        with open(relation_filepath, 'r', encoding='UTF-8') as file:\n","            for line in file:\n","                relation, id_str = line.strip().split()\n","                relation_id = int(id_str)\n","                relation_to_id[relation] = relation_id\n","                id_to_relation[relation_id] = relation\n","\n","        # 保存关系映射\n","        relation_maps = {\n","            'relation_to_id': relation_to_id,\n","            'id_to_relation': id_to_relation\n","        }\n","\n","        with open(os.path.join(output_dir, 'relation_maps.json'), 'w', encoding='utf-8') as f:\n","            json.dump(relation_maps, f, ensure_ascii=False, indent=2)\n","\n","        print(f\"关系映射已保存，关系类型数: {len(relation_to_id)}\")\n","        return relation_to_id, id_to_relation\n","    except Exception as e:\n","        print(f\"处理关系映射时出错: {e}\")\n","        raise\n","\n","def copy_model(model_path, output_dir):\n","    \"\"\"复制模型权重文件\"\"\"\n","    print(f\"正在复制模型权重: {model_path}\")\n","\n","    try:\n","        # 加载模型权重以验证格式\n","        model_weights = torch.load(model_path, map_location='cpu')\n","        # 保存到输出目录\n","        torch.save(model_weights, os.path.join(output_dir, 'final_model.pkl'))\n","        print(\"模型权重已复制\")\n","    except Exception as e:\n","        print(f\"复制模型权重时出错: {e}\")\n","        raise\n","\n","def main():\n","    args = parse_args()\n","\n","    # 创建输出目录\n","    os.makedirs(args.output_dir, exist_ok=True)\n","\n","    # 处理并保存词嵌入\n","    prepare_embeddings(args.embedding_path, args.word_dim, args.output_dir)\n","\n","    # 处理并保存关系映射\n","    prepare_relation_mapping(args.relation_path, args.output_dir)\n","\n","    # 复制模型权重\n","    copy_model(args.model_path, args.output_dir)\n","\n","    print(f\"预处理完成! 所有文件已保存到: {args.output_dir}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xJpKcptkALWm","executionInfo":{"status":"ok","timestamp":1741256970480,"user_tz":0,"elapsed":45,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"93b6cc86-7176-4b22-f670-f68ba31394f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing preprocess.py\n"]}]},{"cell_type":"code","source":["# 创建目录\n","!mkdir -p /content/hf-deployment/model\n","\n","# 运行预处理脚本\n","!python preprocess.py \\\n","  --embedding_path /content/drive/MyDrive/nlp/textmining_CW/embedding/glove.6B.100d.txt \\\n","  --relation_path /content/drive/MyDrive/nlp/textmining_CW/data/relation_with_id.txt \\\n","  --model_path /content/drive/MyDrive/nlp/textmining_CW/output/Att_LSTM/final_model.pkl \\\n","  --output_dir /content/hf-deployment/model \\\n","  --word_dim 100"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8KdXEnFGAP-R","executionInfo":{"status":"ok","timestamp":1741257028947,"user_tz":0,"elapsed":43314,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"6d4f9e1b-1f00-432c-9570-bdf3eebc3f07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["正在处理词嵌入: /content/drive/MyDrive/nlp/textmining_CW/embedding/glove.6B.100d.txt\n","读取词嵌入: 400000it [00:19, 20415.95it/s]\n","词嵌入已保存，词汇量: 400006\n","正在处理关系映射: /content/drive/MyDrive/nlp/textmining_CW/data/relation_with_id.txt\n","关系映射已保存，关系类型数: 19\n","正在复制模型权重: /content/drive/MyDrive/nlp/textmining_CW/output/Att_LSTM/final_model.pkl\n","/content/preprocess.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model_weights = torch.load(model_path, map_location='cpu')\n","模型权重已复制\n","预处理完成! 所有文件已保存到: /content/hf-deployment/model\n"]}]},{"cell_type":"code","source":["%%writefile /content/hf-deployment/requirements.txt\n","torch==2.0.1\n","numpy==1.24.3\n","gradio==3.50.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PfLgmYreAVNV","executionInfo":{"status":"ok","timestamp":1741257031716,"user_tz":0,"elapsed":5,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"949d6dfe-2518-4e58-fe50-865c91cd51ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/hf-deployment/requirements.txt\n"]}]},{"cell_type":"code","source":["%%writefile /content/hf-deployment/README.md\n","---\n","title: LSTM关系抽取模型\n","emoji: 🔍\n","colorFrom: blue\n","colorTo: indigo\n","sdk: gradio\n","sdk_version: 3.50.0\n","app_file: app.py\n","pinned: false\n","license: mit\n","---\n","\n","# LSTM关系抽取模型\n","\n","这个应用使用LSTM+多头注意力机制来预测句子中两个实体之间的关系。\n","\n","## 使用说明\n","\n","输入包含两个实体标记的句子，模型将预测它们之间的关系。\n","\n","### 格式要求\n","- 使用 `<e1>` 和 `</e1>` 标记第一个实体\n","- 使用 `<e2>` 和 `</e2>` 标记第二个实体\n","\n","## 示例\n","\n","- `<e1>John</e1> works at <e2>Google</e2>.`\n","- `<e1>Aspirin</e1> is used to treat <e2>headaches</e2>.`\n","- `<e1>John</e1> is the father of <e2>Alice</e2>.`\n","\n","## 模型架构\n","\n","该模型基于LSTM结合多头注意力机制，使用预训练的GloVe词向量进行初始化。"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eje8pSi6Afe0","executionInfo":{"status":"ok","timestamp":1741257049590,"user_tz":0,"elapsed":7,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"f922dd9e-e2a1-4917-b169-9f6c2d9d1722"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/hf-deployment/README.md\n"]}]},{"cell_type":"code","source":["%%writefile /content/hf-deployment/app.py\n","import gradio as gr\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","import os\n","import numpy as np\n","import json\n","\n","# 配置类\n","class Configuration:\n","    \"\"\"配置类\"\"\"\n","    def __init__(self):\n","        self.model_dir = os.environ.get('MODEL_DIR', './model')\n","        self.word_dim = int(os.environ.get('WORD_DIM', '100'))\n","        self.max_len = int(os.environ.get('MAX_LEN', '100'))\n","        self.hidden_size = int(os.environ.get('HIDDEN_SIZE', '100'))\n","        self.num_heads = int(os.environ.get('NUM_HEADS', '4'))\n","        self.layers_num = int(os.environ.get('LAYERS_NUM', '1'))\n","        self.emb_dropout = float(os.environ.get('EMB_DROPOUT', '0.3'))\n","        self.lstm_dropout = float(os.environ.get('LSTM_DROPOUT', '0.3'))\n","        self.linear_dropout = float(os.environ.get('LINEAR_DROPOUT', '0.5'))\n","\n","        # 检测设备\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# 多头注意力机制模型\n","class EnhancedAttention(nn.Module):\n","    \"\"\"多头注意力机制\"\"\"\n","    def __init__(self, model_dim, head_num, Dropout_Rate=0.1):\n","        super().__init__()\n","        self.head_num = head_num\n","        self.key_dim = model_dim // head_num\n","        self.query_transform = nn.Linear(model_dim, model_dim)\n","        self.key_transform = nn.Linear(model_dim, model_dim)\n","        self.value_transform = nn.Linear(model_dim, model_dim)\n","        self.output_transform = nn.Linear(model_dim, model_dim)\n","        self.dropout = nn.Dropout(Dropout_Rate)\n","\n","    def _calculate_attention(self, query, key, value, mask=None):\n","        attention_scores0 = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.key_dim)\n","        if mask is not None:\n","            attention_scores1 = attention_scores0.masked_fill(mask == 0, -1e9)\n","        else:\n","            attention_scores1 = attention_scores0\n","        attention_weights1 = F.softmax(attention_scores1, dim=-1)\n","        attention_weights2 = self.dropout(attention_weights1)\n","        return torch.matmul(attention_weights2, value)\n","\n","    def _split_heads(self, tensor):\n","        batch_size, seq_len, model_dim = tensor.size()\n","        return tensor.view(batch_size, seq_len, self.head_num, self.key_dim).transpose(1, 2)\n","\n","    def _merge_heads(self, tensor):\n","        batch_size, _, seq_len, key_dim = tensor.size()\n","        return tensor.transpose(1, 2).contiguous().view(batch_size, seq_len, self.head_num * key_dim)\n","\n","    def forward(self, query, key, value, mask=None):\n","        query, key, value = self.query_transform(query), self.key_transform(key), self.value_transform(value)\n","        query_split, key_split, value_split = self._split_heads(query), self._split_heads(key), self._split_heads(value)\n","        if mask is not None:\n","            mask = mask.unsqueeze(1).unsqueeze(1).expand(-1, self.head_num, -1, -1).bool()\n","        output_split = self._calculate_attention(query_split, key_split, value_split, mask)\n","        output = self._merge_heads(output_split)\n","        return self.output_transform(output)\n","\n","# LSTM结合注意力机制的模型\n","class AttentiveLSTM(nn.Module):\n","    \"\"\"LSTM结合注意力机制的模型\"\"\"\n","    def __init__(self, word_embeddings, num_classes, settings):\n","        super().__init__()\n","        self.word_embeddings = word_embeddings\n","        self.num_classes = num_classes\n","        self.max_sequence_length = settings.max_len\n","        self.embedding_dim = settings.word_dim\n","        self.lstm_hidden_size = settings.hidden_size\n","        self.embedding_dropout = nn.Dropout(settings.emb_dropout)\n","        self.lstm_dropout = nn.Dropout(settings.lstm_dropout)\n","        self.linear_dropout = nn.Dropout(settings.linear_dropout)\n","        self.attention_heads = settings.num_heads\n","        self.embedding_layer = nn.Embedding.from_pretrained(embeddings=self.word_embeddings, freeze=False)\n","\n","        self.lstm_layer = nn.LSTM(\n","            input_size=self.embedding_dim,\n","            hidden_size=self.lstm_hidden_size,\n","            batch_first=True,\n","            num_layers=settings.layers_num,\n","            bidirectional=False\n","        )\n","\n","        self.attention_mechanism = EnhancedAttention(self.lstm_hidden_size, self.attention_heads)\n","        self.output_layer = nn.Linear(in_features=self.lstm_hidden_size, out_features=self.num_classes)\n","        # 初始化权重\n","        nn.init.xavier_normal_(self.output_layer.weight)\n","        nn.init.constant_(self.output_layer.bias, 0.)\n","\n","    def _process_with_lstm(self, input_data, mask):\n","        len_total = torch.sum(mask.gt(0), dim=-1).cpu().type(torch.int64)\n","        packinput_inf = nn.utils.rnn.pack_padded_sequence(input_data, len_total, batch_first=True, enforce_sorted=False)\n","        lstm_out, _ = self.lstm_layer(packinput_inf)\n","        unpacked_output, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True, padding_value=0.0, total_length=self.max_sequence_length)\n","        return unpacked_output\n","\n","    def forward(self, input_data):\n","        tokens = input_data[:, 0, :].squeeze(1)\n","        mask = input_data[:, 1, :].squeeze(1)\n","        embedded_tokens = self.embedding_dropout(self.embedding_layer(tokens))\n","        lstm_hide_states = self.lstm_dropout(self._process_with_lstm(embedded_tokens, mask))\n","        attended_states = self.attention_mechanism(lstm_hide_states, lstm_hide_states, lstm_hide_states, mask)\n","        mean_attended_states = torch.mean(attended_states, dim=1)\n","        regularized_states = self.linear_dropout(mean_attended_states)\n","        return self.output_layer(regularized_states)\n","\n","# 全局变量\n","model = None\n","word_to_id = None\n","id_to_relation = None\n","config = None\n","\n","# 加载模型\n","def load_model():\n","    global model, word_to_id, id_to_relation, config\n","\n","    print(\"正在加载模型...\")\n","    config = Configuration()\n","\n","    try:\n","        # 加载词嵌入\n","        embedding_cache_path = os.path.join(config.model_dir, 'word_embeddings.pt')\n","        word_embeddings = torch.load(embedding_cache_path, map_location='cpu')\n","\n","        # 加载字典\n","        with open(os.path.join(config.model_dir, 'word_to_id.json'), 'r', encoding='utf-8') as f:\n","            word_to_id = json.load(f)\n","\n","        # 加载关系映射\n","        relation_cache_path = os.path.join(config.model_dir, 'relation_maps.json')\n","        with open(relation_cache_path, 'r', encoding='utf-8') as f:\n","            relation_data = json.load(f)\n","\n","        # 将字符串键转回整数\n","        id_to_relation = {int(k): v for k, v in relation_data['id_to_relation'].items()}\n","        relation_to_id = relation_data['relation_to_id']\n","\n","        # 初始化模型\n","        model = AttentiveLSTM(\n","            word_embeddings=word_embeddings,\n","            num_classes=len(relation_to_id),\n","            settings=config\n","        ).to(config.device)\n","\n","        # 加载模型权重\n","        model_path = os.path.join(config.model_dir, 'final_model.pkl')\n","        model.load_state_dict(torch.load(model_path, map_location=config.device))\n","        model.eval()\n","\n","        print(f\"模型已成功加载到 {config.device} 设备\")\n","        return True\n","    except Exception as e:\n","        print(f\"模型加载失败: {str(e)}\")\n","        return False\n","\n","# 预处理输入句子\n","def prepare_input_sentence(sentence, word_to_id, max_length):\n","    \"\"\"将输入句子转换为模型可用的格式\"\"\"\n","    tokens = sentence.strip().split()\n","    token_ids = [word_to_id.get(token.lower(), word_to_id['UNK']) for token in tokens]\n","\n","    # 填充或截断到固定长度\n","    if len(token_ids) < max_length:\n","        token_ids_padded = token_ids + [word_to_id['PAD']] * (max_length - len(token_ids))\n","    else:\n","        token_ids_padded = token_ids[:max_length]\n","\n","    # 创建掩码\n","    mask = [1] * min(len(tokens), max_length) + [0] * (max_length - min(len(tokens), max_length))\n","\n","    # 转换为张量\n","    tokens_tensor = torch.tensor([token_ids_padded], dtype=torch.long)\n","    mask_tensor = torch.tensor([mask], dtype=torch.long)\n","\n","    # 堆叠为模型输入格式\n","    data = torch.stack([tokens_tensor, mask_tensor], dim=1)\n","    return data\n","\n","# 预测函数\n","def predict_relation(sentence):\n","    global model, word_to_id, id_to_relation, config\n","\n","    # 检查模型是否已加载\n","    if model is None:\n","        success = load_model()\n","        if not success:\n","            return \"错误：模型加载失败，请检查模型文件\"\n","\n","    # 检查输入\n","    if not \"<e1>\" in sentence or not \"<e2>\" in sentence:\n","        return \"错误：句子必须包含实体标记，例如: '<e1>John</e1> works at <e2>Google</e2>.'\"\n","\n","    try:\n","        # 准备输入数据\n","        data = prepare_input_sentence(sentence, word_to_id, config.max_len).to(config.device)\n","\n","        # 预测\n","        with torch.no_grad():\n","            logits = model(data)\n","            probabilities = F.softmax(logits, dim=1)\n","            max_prob, prediction = torch.max(probabilities, dim=1)\n","            pred_idx = prediction.cpu().item()\n","            confidence = max_prob.cpu().item()\n","\n","        # 返回结果\n","        relation_type = id_to_relation[pred_idx]\n","        confidence_percent = confidence * 100\n","\n","        # 格式化输出结果\n","        result = f\"预测关系: {relation_type}\\n\"\n","        result += f\"置信度: {confidence_percent:.2f}%\\n\\n\"\n","\n","        # 显示所有关系的概率分布\n","        result += \"所有关系的概率分布:\\n\"\n","        probs = probabilities[0].cpu().numpy()\n","        for idx, prob in enumerate(probs):\n","            if idx in id_to_relation:\n","                rel_name = id_to_relation[idx]\n","                result += f\"- {rel_name}: {prob*100:.2f}%\\n\"\n","\n","        return result\n","    except Exception as e:\n","        return f\"预测过程中出错: {str(e)}\"\n","\n","# 尝试加载模型\n","load_model()\n","\n","# 创建Gradio界面\n","demo = gr.Interface(\n","    fn=predict_relation,\n","    inputs=gr.Textbox(\n","        placeholder=\"例如：<e1>John</e1> works at <e2>Google</e2>.\",\n","        label=\"输入句子\"\n","    ),\n","    outputs=gr.Textbox(label=\"预测结果\"),\n","    title=\"LSTM关系抽取模型\",\n","    description=\"\"\"\n","    ## 使用说明\n","    这个应用使用LSTM+注意力机制来预测句子中两个实体之间的关系。\n","\n","    请输入包含两个实体标记的句子，模型将预测它们之间的关系。\n","\n","    **格式要求**：\n","    - 使用 `<e1>` 和 `</e1>` 标记第一个实体\n","    - 使用 `<e2>` 和 `</e2>` 标记第二个实体\n","    \"\"\",\n","    examples=[\n","        [\"<e1>John</e1> works at <e2>Google</e2>.\"],\n","        [\"<e1>Aspirin</e1> is used to treat <e2>headaches</e2>.\"],\n","        [\"<e1>John</e1> is the father of <e2>Alice</e2>.\"],\n","        [\"<e1>The book</e1> is on <e2>the table</e2>.\"]\n","    ],\n","    theme=gr.themes.Soft()\n",")\n","\n","# 启动界面\n","if __name__ == \"__main__\":\n","    demo.launch(share=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W0G47TCOAivx","executionInfo":{"status":"ok","timestamp":1741257505672,"user_tz":0,"elapsed":16,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"763fa55a-658a-4bde-da37-7c99e5b2e04b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/hf-deployment/app.py\n"]}]},{"cell_type":"code","source":["%%writefile /content/hf-deployment/.gitattributes\n","*.pt filter=lfs diff=lfs merge=lfs -text\n","*.pkl filter=lfs diff=lfs merge=lfs -text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kCbyAp2vAqW3","executionInfo":{"status":"ok","timestamp":1741257511321,"user_tz":0,"elapsed":45,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"f85b74eb-3278-4566-90b4-83dd3d2e5de4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/hf-deployment/.gitattributes\n"]}]},{"cell_type":"code","source":["%cd /content/hf-deployment\n","!python app.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Criyf-WUAu5t","executionInfo":{"status":"ok","timestamp":1741257605295,"user_tz":0,"elapsed":92512,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"955a6969-9a46-4a43-b0b1-e267efe57f15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/hf-deployment\n","正在加载模型...\n","/content/hf-deployment/app.py:130: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  word_embeddings = torch.load(embedding_cache_path, map_location='cpu')\n","/content/hf-deployment/app.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_path, map_location=config.device))\n","模型已成功加载到 cpu 设备\n","* Running on local URL:  http://127.0.0.1:7860\n","* Running on public URL: https://f776f54a6af9214194.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","Keyboard interruption in main thread... closing server.\n","Killing tunnel 127.0.0.1:7860 <> https://f776f54a6af9214194.gradio.live\n"]}]},{"cell_type":"code","source":["!cd /content && zip -r hf-deployment.zip hf-deployment/\n","from google.colab import files\n","files.download('/content/hf-deployment.zip')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":243},"id":"r9WNjOxAC1q2","executionInfo":{"status":"ok","timestamp":1741257686543,"user_tz":0,"elapsed":23106,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"024b380a-6c5b-422e-ba81-279596bfa87c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: hf-deployment/ (stored 0%)\n","  adding: hf-deployment/.gitattributes (deflated 47%)\n","  adding: hf-deployment/README.md (deflated 34%)\n","  adding: hf-deployment/model/ (stored 0%)\n","  adding: hf-deployment/model/word_to_id.json (deflated 64%)\n","  adding: hf-deployment/model/relation_maps.json (deflated 74%)\n","  adding: hf-deployment/model/final_model.pkl (deflated 8%)\n","  adding: hf-deployment/model/word_embeddings.pt (deflated 8%)\n","  adding: hf-deployment/requirements.txt (stored 0%)\n","  adding: hf-deployment/.gradio/ (stored 0%)\n","  adding: hf-deployment/.gradio/certificate.pem (deflated 24%)\n","  adding: hf-deployment/app.py (deflated 67%)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_e7a45ed0-5e6a-48c2-a291-e673deab45bd\", \"hf-deployment.zip\", 296599184)"]},"metadata":{}}]}]}