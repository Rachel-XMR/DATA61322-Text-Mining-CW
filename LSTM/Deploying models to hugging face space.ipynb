{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO1wkpIfvwR5QpI2/f0UAkY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8tMd_jkcADUf","executionInfo":{"status":"ok","timestamp":1741256960100,"user_tz":0,"elapsed":15543,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"d20082e8-135c-49ae-f4ff-f3b92e6ac1fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install gradio"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4E1taIFZBJsy","executionInfo":{"status":"ok","timestamp":1741257234848,"user_tz":0,"elapsed":14618,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"ad72c604-8434-44d6-b44a-2f122cf162ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gradio\n","  Downloading gradio-5.20.0-py3-none-any.whl.metadata (16 kB)\n","Collecting aiofiles<24.0,>=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n","Collecting fastapi<1.0,>=0.115.2 (from gradio)\n","  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n","Collecting ffmpy (from gradio)\n","  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n","Collecting gradio-client==1.7.2 (from gradio)\n","  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting groovy~=0.1 (from gradio)\n","  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n","Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n","Collecting markupsafe~=2.0 (from gradio)\n","  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n","Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n","Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n","Collecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting python-multipart>=0.0.18 (from gradio)\n","  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n","Collecting ruff>=0.9.3 (from gradio)\n","  Downloading ruff-0.9.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n","  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n","Collecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting starlette<1.0,>=0.40.0 (from gradio)\n","  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\n","Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n","  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n","Collecting uvicorn>=0.14.0 (from gradio)\n","  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n","Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","Downloading gradio-5.20.0-py3-none-any.whl (62.3 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n","Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n","Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n","Downloading ruff-0.9.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n","Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading starlette-0.46.0-py3-none-any.whl (71 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n","Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n","Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n","  Attempting uninstall: markupsafe\n","    Found existing installation: MarkupSafe 3.0.2\n","    Uninstalling MarkupSafe-3.0.2:\n","      Successfully uninstalled MarkupSafe-3.0.2\n","Successfully installed aiofiles-23.2.1 fastapi-0.115.11 ffmpy-0.5.0 gradio-5.20.0 gradio-client-1.7.2 groovy-0.1.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.9 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.0 tomlkit-0.13.2 uvicorn-0.34.0\n"]}]},{"cell_type":"code","source":["%%writefile preprocess.py\n","import os\n","import torch\n","import numpy as np\n","import json\n","import argparse\n","from tqdm import tqdm\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser(description='å‡†å¤‡æ¨¡å‹éƒ¨ç½²æ‰€éœ€æ–‡ä»¶')\n","    parser.add_argument('--embedding_path', type=str, required=True, help='GloVeè¯åµŒå…¥æ–‡ä»¶è·¯å¾„')\n","    parser.add_argument('--relation_path', type=str, required=True, help='å…³ç³»æ˜ å°„æ–‡ä»¶è·¯å¾„')\n","    parser.add_argument('--model_path', type=str, required=True, help='è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„')\n","    parser.add_argument('--output_dir', type=str, required=True, help='è¾“å‡ºç›®å½•')\n","    parser.add_argument('--word_dim', type=int, default=100, help='è¯åµŒå…¥ç»´åº¦')\n","    return parser.parse_args()\n","\n","def prepare_embeddings(embedding_filepath, embedding_dimension, output_dir):\n","    \"\"\"å¤„ç†å¹¶ä¿å­˜è¯åµŒå…¥\"\"\"\n","    print(f\"æ­£åœ¨å¤„ç†è¯åµŒå…¥: {embedding_filepath}\")\n","\n","    word_to_id = {'PAD': 0, 'UNK': 1, '<e1>': 2, '<e2>': 3, '</e1>': 4, '</e2>': 5}\n","    word_vectors = []\n","\n","    # è¯»å–GloVeè¯åµŒå…¥\n","    with open(embedding_filepath, 'r', encoding='UTF-8') as file:\n","        for line in tqdm(file, desc=\"è¯»å–è¯åµŒå…¥\"):\n","            elements = line.strip().split()\n","            if len(elements) == embedding_dimension + 1:\n","                word, vector = elements[0], np.array(elements[1:], dtype=np.float32)\n","                word_to_id[word] = len(word_to_id)\n","                word_vectors.append(vector)\n","\n","    # å¤„ç†è¯åµŒå…¥\n","    word_vectors = np.stack(word_vectors)\n","    vector_mean, vector_std = word_vectors.mean(), word_vectors.std()\n","    special_vectors = np.random.normal(vector_mean, vector_std, (6, embedding_dimension))\n","    special_vectors[0] = 0  # PADå‘é‡è®¾ä¸º0\n","    word_vectors = np.concatenate((special_vectors, word_vectors), axis=0)\n","\n","    # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶ä¿å­˜\n","    word_embeddings = torch.from_numpy(word_vectors.astype(np.float32))\n","    torch.save(word_embeddings, os.path.join(output_dir, 'word_embeddings.pt'))\n","\n","    # ä¿å­˜word_to_idæ˜ å°„\n","    with open(os.path.join(output_dir, 'word_to_id.json'), 'w', encoding='utf-8') as f:\n","        json.dump(word_to_id, f, ensure_ascii=False, indent=2)\n","\n","    print(f\"è¯åµŒå…¥å·²ä¿å­˜ï¼Œè¯æ±‡é‡: {len(word_to_id)}\")\n","    return word_to_id, word_embeddings\n","\n","def prepare_relation_mapping(relation_filepath, output_dir):\n","    \"\"\"å¤„ç†å¹¶ä¿å­˜å…³ç³»æ˜ å°„\"\"\"\n","    print(f\"æ­£åœ¨å¤„ç†å…³ç³»æ˜ å°„: {relation_filepath}\")\n","\n","    relation_to_id, id_to_relation = {}, {}\n","\n","    try:\n","        with open(relation_filepath, 'r', encoding='UTF-8') as file:\n","            for line in file:\n","                relation, id_str = line.strip().split()\n","                relation_id = int(id_str)\n","                relation_to_id[relation] = relation_id\n","                id_to_relation[relation_id] = relation\n","\n","        # ä¿å­˜å…³ç³»æ˜ å°„\n","        relation_maps = {\n","            'relation_to_id': relation_to_id,\n","            'id_to_relation': id_to_relation\n","        }\n","\n","        with open(os.path.join(output_dir, 'relation_maps.json'), 'w', encoding='utf-8') as f:\n","            json.dump(relation_maps, f, ensure_ascii=False, indent=2)\n","\n","        print(f\"å…³ç³»æ˜ å°„å·²ä¿å­˜ï¼Œå…³ç³»ç±»å‹æ•°: {len(relation_to_id)}\")\n","        return relation_to_id, id_to_relation\n","    except Exception as e:\n","        print(f\"å¤„ç†å…³ç³»æ˜ å°„æ—¶å‡ºé”™: {e}\")\n","        raise\n","\n","def copy_model(model_path, output_dir):\n","    \"\"\"å¤åˆ¶æ¨¡å‹æƒé‡æ–‡ä»¶\"\"\"\n","    print(f\"æ­£åœ¨å¤åˆ¶æ¨¡å‹æƒé‡: {model_path}\")\n","\n","    try:\n","        # åŠ è½½æ¨¡å‹æƒé‡ä»¥éªŒè¯æ ¼å¼\n","        model_weights = torch.load(model_path, map_location='cpu')\n","        # ä¿å­˜åˆ°è¾“å‡ºç›®å½•\n","        torch.save(model_weights, os.path.join(output_dir, 'final_model.pkl'))\n","        print(\"æ¨¡å‹æƒé‡å·²å¤åˆ¶\")\n","    except Exception as e:\n","        print(f\"å¤åˆ¶æ¨¡å‹æƒé‡æ—¶å‡ºé”™: {e}\")\n","        raise\n","\n","def main():\n","    args = parse_args()\n","\n","    # åˆ›å»ºè¾“å‡ºç›®å½•\n","    os.makedirs(args.output_dir, exist_ok=True)\n","\n","    # å¤„ç†å¹¶ä¿å­˜è¯åµŒå…¥\n","    prepare_embeddings(args.embedding_path, args.word_dim, args.output_dir)\n","\n","    # å¤„ç†å¹¶ä¿å­˜å…³ç³»æ˜ å°„\n","    prepare_relation_mapping(args.relation_path, args.output_dir)\n","\n","    # å¤åˆ¶æ¨¡å‹æƒé‡\n","    copy_model(args.model_path, args.output_dir)\n","\n","    print(f\"é¢„å¤„ç†å®Œæˆ! æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {args.output_dir}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xJpKcptkALWm","executionInfo":{"status":"ok","timestamp":1741256970480,"user_tz":0,"elapsed":45,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"93b6cc86-7176-4b22-f670-f68ba31394f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing preprocess.py\n"]}]},{"cell_type":"code","source":["# åˆ›å»ºç›®å½•\n","!mkdir -p /content/hf-deployment/model\n","\n","# è¿è¡Œé¢„å¤„ç†è„šæœ¬\n","!python preprocess.py \\\n","  --embedding_path /content/drive/MyDrive/nlp/textmining_CW/embedding/glove.6B.100d.txt \\\n","  --relation_path /content/drive/MyDrive/nlp/textmining_CW/data/relation_with_id.txt \\\n","  --model_path /content/drive/MyDrive/nlp/textmining_CW/output/Att_LSTM/final_model.pkl \\\n","  --output_dir /content/hf-deployment/model \\\n","  --word_dim 100"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8KdXEnFGAP-R","executionInfo":{"status":"ok","timestamp":1741257028947,"user_tz":0,"elapsed":43314,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"6d4f9e1b-1f00-432c-9570-bdf3eebc3f07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["æ­£åœ¨å¤„ç†è¯åµŒå…¥: /content/drive/MyDrive/nlp/textmining_CW/embedding/glove.6B.100d.txt\n","è¯»å–è¯åµŒå…¥: 400000it [00:19, 20415.95it/s]\n","è¯åµŒå…¥å·²ä¿å­˜ï¼Œè¯æ±‡é‡: 400006\n","æ­£åœ¨å¤„ç†å…³ç³»æ˜ å°„: /content/drive/MyDrive/nlp/textmining_CW/data/relation_with_id.txt\n","å…³ç³»æ˜ å°„å·²ä¿å­˜ï¼Œå…³ç³»ç±»å‹æ•°: 19\n","æ­£åœ¨å¤åˆ¶æ¨¡å‹æƒé‡: /content/drive/MyDrive/nlp/textmining_CW/output/Att_LSTM/final_model.pkl\n","/content/preprocess.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model_weights = torch.load(model_path, map_location='cpu')\n","æ¨¡å‹æƒé‡å·²å¤åˆ¶\n","é¢„å¤„ç†å®Œæˆ! æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: /content/hf-deployment/model\n"]}]},{"cell_type":"code","source":["%%writefile /content/hf-deployment/requirements.txt\n","torch==2.0.1\n","numpy==1.24.3\n","gradio==3.50.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PfLgmYreAVNV","executionInfo":{"status":"ok","timestamp":1741257031716,"user_tz":0,"elapsed":5,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"949d6dfe-2518-4e58-fe50-865c91cd51ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/hf-deployment/requirements.txt\n"]}]},{"cell_type":"code","source":["%%writefile /content/hf-deployment/README.md\n","---\n","title: LSTMå…³ç³»æŠ½å–æ¨¡å‹\n","emoji: ğŸ”\n","colorFrom: blue\n","colorTo: indigo\n","sdk: gradio\n","sdk_version: 3.50.0\n","app_file: app.py\n","pinned: false\n","license: mit\n","---\n","\n","# LSTMå…³ç³»æŠ½å–æ¨¡å‹\n","\n","è¿™ä¸ªåº”ç”¨ä½¿ç”¨LSTM+å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æ¥é¢„æµ‹å¥å­ä¸­ä¸¤ä¸ªå®ä½“ä¹‹é—´çš„å…³ç³»ã€‚\n","\n","## ä½¿ç”¨è¯´æ˜\n","\n","è¾“å…¥åŒ…å«ä¸¤ä¸ªå®ä½“æ ‡è®°çš„å¥å­ï¼Œæ¨¡å‹å°†é¢„æµ‹å®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚\n","\n","### æ ¼å¼è¦æ±‚\n","- ä½¿ç”¨ `<e1>` å’Œ `</e1>` æ ‡è®°ç¬¬ä¸€ä¸ªå®ä½“\n","- ä½¿ç”¨ `<e2>` å’Œ `</e2>` æ ‡è®°ç¬¬äºŒä¸ªå®ä½“\n","\n","## ç¤ºä¾‹\n","\n","- `<e1>John</e1> works at <e2>Google</e2>.`\n","- `<e1>Aspirin</e1> is used to treat <e2>headaches</e2>.`\n","- `<e1>John</e1> is the father of <e2>Alice</e2>.`\n","\n","## æ¨¡å‹æ¶æ„\n","\n","è¯¥æ¨¡å‹åŸºäºLSTMç»“åˆå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„GloVeè¯å‘é‡è¿›è¡Œåˆå§‹åŒ–ã€‚"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eje8pSi6Afe0","executionInfo":{"status":"ok","timestamp":1741257049590,"user_tz":0,"elapsed":7,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"f922dd9e-e2a1-4917-b169-9f6c2d9d1722"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/hf-deployment/README.md\n"]}]},{"cell_type":"code","source":["%%writefile /content/hf-deployment/app.py\n","import gradio as gr\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","import os\n","import numpy as np\n","import json\n","\n","# é…ç½®ç±»\n","class Configuration:\n","    \"\"\"é…ç½®ç±»\"\"\"\n","    def __init__(self):\n","        self.model_dir = os.environ.get('MODEL_DIR', './model')\n","        self.word_dim = int(os.environ.get('WORD_DIM', '100'))\n","        self.max_len = int(os.environ.get('MAX_LEN', '100'))\n","        self.hidden_size = int(os.environ.get('HIDDEN_SIZE', '100'))\n","        self.num_heads = int(os.environ.get('NUM_HEADS', '4'))\n","        self.layers_num = int(os.environ.get('LAYERS_NUM', '1'))\n","        self.emb_dropout = float(os.environ.get('EMB_DROPOUT', '0.3'))\n","        self.lstm_dropout = float(os.environ.get('LSTM_DROPOUT', '0.3'))\n","        self.linear_dropout = float(os.environ.get('LINEAR_DROPOUT', '0.5'))\n","\n","        # æ£€æµ‹è®¾å¤‡\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æ¨¡å‹\n","class EnhancedAttention(nn.Module):\n","    \"\"\"å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶\"\"\"\n","    def __init__(self, model_dim, head_num, Dropout_Rate=0.1):\n","        super().__init__()\n","        self.head_num = head_num\n","        self.key_dim = model_dim // head_num\n","        self.query_transform = nn.Linear(model_dim, model_dim)\n","        self.key_transform = nn.Linear(model_dim, model_dim)\n","        self.value_transform = nn.Linear(model_dim, model_dim)\n","        self.output_transform = nn.Linear(model_dim, model_dim)\n","        self.dropout = nn.Dropout(Dropout_Rate)\n","\n","    def _calculate_attention(self, query, key, value, mask=None):\n","        attention_scores0 = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.key_dim)\n","        if mask is not None:\n","            attention_scores1 = attention_scores0.masked_fill(mask == 0, -1e9)\n","        else:\n","            attention_scores1 = attention_scores0\n","        attention_weights1 = F.softmax(attention_scores1, dim=-1)\n","        attention_weights2 = self.dropout(attention_weights1)\n","        return torch.matmul(attention_weights2, value)\n","\n","    def _split_heads(self, tensor):\n","        batch_size, seq_len, model_dim = tensor.size()\n","        return tensor.view(batch_size, seq_len, self.head_num, self.key_dim).transpose(1, 2)\n","\n","    def _merge_heads(self, tensor):\n","        batch_size, _, seq_len, key_dim = tensor.size()\n","        return tensor.transpose(1, 2).contiguous().view(batch_size, seq_len, self.head_num * key_dim)\n","\n","    def forward(self, query, key, value, mask=None):\n","        query, key, value = self.query_transform(query), self.key_transform(key), self.value_transform(value)\n","        query_split, key_split, value_split = self._split_heads(query), self._split_heads(key), self._split_heads(value)\n","        if mask is not None:\n","            mask = mask.unsqueeze(1).unsqueeze(1).expand(-1, self.head_num, -1, -1).bool()\n","        output_split = self._calculate_attention(query_split, key_split, value_split, mask)\n","        output = self._merge_heads(output_split)\n","        return self.output_transform(output)\n","\n","# LSTMç»“åˆæ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹\n","class AttentiveLSTM(nn.Module):\n","    \"\"\"LSTMç»“åˆæ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹\"\"\"\n","    def __init__(self, word_embeddings, num_classes, settings):\n","        super().__init__()\n","        self.word_embeddings = word_embeddings\n","        self.num_classes = num_classes\n","        self.max_sequence_length = settings.max_len\n","        self.embedding_dim = settings.word_dim\n","        self.lstm_hidden_size = settings.hidden_size\n","        self.embedding_dropout = nn.Dropout(settings.emb_dropout)\n","        self.lstm_dropout = nn.Dropout(settings.lstm_dropout)\n","        self.linear_dropout = nn.Dropout(settings.linear_dropout)\n","        self.attention_heads = settings.num_heads\n","        self.embedding_layer = nn.Embedding.from_pretrained(embeddings=self.word_embeddings, freeze=False)\n","\n","        self.lstm_layer = nn.LSTM(\n","            input_size=self.embedding_dim,\n","            hidden_size=self.lstm_hidden_size,\n","            batch_first=True,\n","            num_layers=settings.layers_num,\n","            bidirectional=False\n","        )\n","\n","        self.attention_mechanism = EnhancedAttention(self.lstm_hidden_size, self.attention_heads)\n","        self.output_layer = nn.Linear(in_features=self.lstm_hidden_size, out_features=self.num_classes)\n","        # åˆå§‹åŒ–æƒé‡\n","        nn.init.xavier_normal_(self.output_layer.weight)\n","        nn.init.constant_(self.output_layer.bias, 0.)\n","\n","    def _process_with_lstm(self, input_data, mask):\n","        len_total = torch.sum(mask.gt(0), dim=-1).cpu().type(torch.int64)\n","        packinput_inf = nn.utils.rnn.pack_padded_sequence(input_data, len_total, batch_first=True, enforce_sorted=False)\n","        lstm_out, _ = self.lstm_layer(packinput_inf)\n","        unpacked_output, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True, padding_value=0.0, total_length=self.max_sequence_length)\n","        return unpacked_output\n","\n","    def forward(self, input_data):\n","        tokens = input_data[:, 0, :].squeeze(1)\n","        mask = input_data[:, 1, :].squeeze(1)\n","        embedded_tokens = self.embedding_dropout(self.embedding_layer(tokens))\n","        lstm_hide_states = self.lstm_dropout(self._process_with_lstm(embedded_tokens, mask))\n","        attended_states = self.attention_mechanism(lstm_hide_states, lstm_hide_states, lstm_hide_states, mask)\n","        mean_attended_states = torch.mean(attended_states, dim=1)\n","        regularized_states = self.linear_dropout(mean_attended_states)\n","        return self.output_layer(regularized_states)\n","\n","# å…¨å±€å˜é‡\n","model = None\n","word_to_id = None\n","id_to_relation = None\n","config = None\n","\n","# åŠ è½½æ¨¡å‹\n","def load_model():\n","    global model, word_to_id, id_to_relation, config\n","\n","    print(\"æ­£åœ¨åŠ è½½æ¨¡å‹...\")\n","    config = Configuration()\n","\n","    try:\n","        # åŠ è½½è¯åµŒå…¥\n","        embedding_cache_path = os.path.join(config.model_dir, 'word_embeddings.pt')\n","        word_embeddings = torch.load(embedding_cache_path, map_location='cpu')\n","\n","        # åŠ è½½å­—å…¸\n","        with open(os.path.join(config.model_dir, 'word_to_id.json'), 'r', encoding='utf-8') as f:\n","            word_to_id = json.load(f)\n","\n","        # åŠ è½½å…³ç³»æ˜ å°„\n","        relation_cache_path = os.path.join(config.model_dir, 'relation_maps.json')\n","        with open(relation_cache_path, 'r', encoding='utf-8') as f:\n","            relation_data = json.load(f)\n","\n","        # å°†å­—ç¬¦ä¸²é”®è½¬å›æ•´æ•°\n","        id_to_relation = {int(k): v for k, v in relation_data['id_to_relation'].items()}\n","        relation_to_id = relation_data['relation_to_id']\n","\n","        # åˆå§‹åŒ–æ¨¡å‹\n","        model = AttentiveLSTM(\n","            word_embeddings=word_embeddings,\n","            num_classes=len(relation_to_id),\n","            settings=config\n","        ).to(config.device)\n","\n","        # åŠ è½½æ¨¡å‹æƒé‡\n","        model_path = os.path.join(config.model_dir, 'final_model.pkl')\n","        model.load_state_dict(torch.load(model_path, map_location=config.device))\n","        model.eval()\n","\n","        print(f\"æ¨¡å‹å·²æˆåŠŸåŠ è½½åˆ° {config.device} è®¾å¤‡\")\n","        return True\n","    except Exception as e:\n","        print(f\"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}\")\n","        return False\n","\n","# é¢„å¤„ç†è¾“å…¥å¥å­\n","def prepare_input_sentence(sentence, word_to_id, max_length):\n","    \"\"\"å°†è¾“å…¥å¥å­è½¬æ¢ä¸ºæ¨¡å‹å¯ç”¨çš„æ ¼å¼\"\"\"\n","    tokens = sentence.strip().split()\n","    token_ids = [word_to_id.get(token.lower(), word_to_id['UNK']) for token in tokens]\n","\n","    # å¡«å……æˆ–æˆªæ–­åˆ°å›ºå®šé•¿åº¦\n","    if len(token_ids) < max_length:\n","        token_ids_padded = token_ids + [word_to_id['PAD']] * (max_length - len(token_ids))\n","    else:\n","        token_ids_padded = token_ids[:max_length]\n","\n","    # åˆ›å»ºæ©ç \n","    mask = [1] * min(len(tokens), max_length) + [0] * (max_length - min(len(tokens), max_length))\n","\n","    # è½¬æ¢ä¸ºå¼ é‡\n","    tokens_tensor = torch.tensor([token_ids_padded], dtype=torch.long)\n","    mask_tensor = torch.tensor([mask], dtype=torch.long)\n","\n","    # å †å ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼\n","    data = torch.stack([tokens_tensor, mask_tensor], dim=1)\n","    return data\n","\n","# é¢„æµ‹å‡½æ•°\n","def predict_relation(sentence):\n","    global model, word_to_id, id_to_relation, config\n","\n","    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½\n","    if model is None:\n","        success = load_model()\n","        if not success:\n","            return \"é”™è¯¯ï¼šæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶\"\n","\n","    # æ£€æŸ¥è¾“å…¥\n","    if not \"<e1>\" in sentence or not \"<e2>\" in sentence:\n","        return \"é”™è¯¯ï¼šå¥å­å¿…é¡»åŒ…å«å®ä½“æ ‡è®°ï¼Œä¾‹å¦‚: '<e1>John</e1> works at <e2>Google</e2>.'\"\n","\n","    try:\n","        # å‡†å¤‡è¾“å…¥æ•°æ®\n","        data = prepare_input_sentence(sentence, word_to_id, config.max_len).to(config.device)\n","\n","        # é¢„æµ‹\n","        with torch.no_grad():\n","            logits = model(data)\n","            probabilities = F.softmax(logits, dim=1)\n","            max_prob, prediction = torch.max(probabilities, dim=1)\n","            pred_idx = prediction.cpu().item()\n","            confidence = max_prob.cpu().item()\n","\n","        # è¿”å›ç»“æœ\n","        relation_type = id_to_relation[pred_idx]\n","        confidence_percent = confidence * 100\n","\n","        # æ ¼å¼åŒ–è¾“å‡ºç»“æœ\n","        result = f\"é¢„æµ‹å…³ç³»: {relation_type}\\n\"\n","        result += f\"ç½®ä¿¡åº¦: {confidence_percent:.2f}%\\n\\n\"\n","\n","        # æ˜¾ç¤ºæ‰€æœ‰å…³ç³»çš„æ¦‚ç‡åˆ†å¸ƒ\n","        result += \"æ‰€æœ‰å…³ç³»çš„æ¦‚ç‡åˆ†å¸ƒ:\\n\"\n","        probs = probabilities[0].cpu().numpy()\n","        for idx, prob in enumerate(probs):\n","            if idx in id_to_relation:\n","                rel_name = id_to_relation[idx]\n","                result += f\"- {rel_name}: {prob*100:.2f}%\\n\"\n","\n","        return result\n","    except Exception as e:\n","        return f\"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}\"\n","\n","# å°è¯•åŠ è½½æ¨¡å‹\n","load_model()\n","\n","# åˆ›å»ºGradioç•Œé¢\n","demo = gr.Interface(\n","    fn=predict_relation,\n","    inputs=gr.Textbox(\n","        placeholder=\"ä¾‹å¦‚ï¼š<e1>John</e1> works at <e2>Google</e2>.\",\n","        label=\"è¾“å…¥å¥å­\"\n","    ),\n","    outputs=gr.Textbox(label=\"é¢„æµ‹ç»“æœ\"),\n","    title=\"LSTMå…³ç³»æŠ½å–æ¨¡å‹\",\n","    description=\"\"\"\n","    ## ä½¿ç”¨è¯´æ˜\n","    è¿™ä¸ªåº”ç”¨ä½¿ç”¨LSTM+æ³¨æ„åŠ›æœºåˆ¶æ¥é¢„æµ‹å¥å­ä¸­ä¸¤ä¸ªå®ä½“ä¹‹é—´çš„å…³ç³»ã€‚\n","\n","    è¯·è¾“å…¥åŒ…å«ä¸¤ä¸ªå®ä½“æ ‡è®°çš„å¥å­ï¼Œæ¨¡å‹å°†é¢„æµ‹å®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚\n","\n","    **æ ¼å¼è¦æ±‚**ï¼š\n","    - ä½¿ç”¨ `<e1>` å’Œ `</e1>` æ ‡è®°ç¬¬ä¸€ä¸ªå®ä½“\n","    - ä½¿ç”¨ `<e2>` å’Œ `</e2>` æ ‡è®°ç¬¬äºŒä¸ªå®ä½“\n","    \"\"\",\n","    examples=[\n","        [\"<e1>John</e1> works at <e2>Google</e2>.\"],\n","        [\"<e1>Aspirin</e1> is used to treat <e2>headaches</e2>.\"],\n","        [\"<e1>John</e1> is the father of <e2>Alice</e2>.\"],\n","        [\"<e1>The book</e1> is on <e2>the table</e2>.\"]\n","    ],\n","    theme=gr.themes.Soft()\n",")\n","\n","# å¯åŠ¨ç•Œé¢\n","if __name__ == \"__main__\":\n","    demo.launch(share=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W0G47TCOAivx","executionInfo":{"status":"ok","timestamp":1741257505672,"user_tz":0,"elapsed":16,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"763fa55a-658a-4bde-da37-7c99e5b2e04b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/hf-deployment/app.py\n"]}]},{"cell_type":"code","source":["%%writefile /content/hf-deployment/.gitattributes\n","*.pt filter=lfs diff=lfs merge=lfs -text\n","*.pkl filter=lfs diff=lfs merge=lfs -text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kCbyAp2vAqW3","executionInfo":{"status":"ok","timestamp":1741257511321,"user_tz":0,"elapsed":45,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"f85b74eb-3278-4566-90b4-83dd3d2e5de4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/hf-deployment/.gitattributes\n"]}]},{"cell_type":"code","source":["%cd /content/hf-deployment\n","!python app.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Criyf-WUAu5t","executionInfo":{"status":"ok","timestamp":1741257605295,"user_tz":0,"elapsed":92512,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"955a6969-9a46-4a43-b0b1-e267efe57f15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/hf-deployment\n","æ­£åœ¨åŠ è½½æ¨¡å‹...\n","/content/hf-deployment/app.py:130: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  word_embeddings = torch.load(embedding_cache_path, map_location='cpu')\n","/content/hf-deployment/app.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_path, map_location=config.device))\n","æ¨¡å‹å·²æˆåŠŸåŠ è½½åˆ° cpu è®¾å¤‡\n","* Running on local URL:  http://127.0.0.1:7860\n","* Running on public URL: https://f776f54a6af9214194.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","Keyboard interruption in main thread... closing server.\n","Killing tunnel 127.0.0.1:7860 <> https://f776f54a6af9214194.gradio.live\n"]}]},{"cell_type":"code","source":["!cd /content && zip -r hf-deployment.zip hf-deployment/\n","from google.colab import files\n","files.download('/content/hf-deployment.zip')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":243},"id":"r9WNjOxAC1q2","executionInfo":{"status":"ok","timestamp":1741257686543,"user_tz":0,"elapsed":23106,"user":{"displayName":"Michelle Clark","userId":"15687893848474844040"}},"outputId":"024b380a-6c5b-422e-ba81-279596bfa87c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: hf-deployment/ (stored 0%)\n","  adding: hf-deployment/.gitattributes (deflated 47%)\n","  adding: hf-deployment/README.md (deflated 34%)\n","  adding: hf-deployment/model/ (stored 0%)\n","  adding: hf-deployment/model/word_to_id.json (deflated 64%)\n","  adding: hf-deployment/model/relation_maps.json (deflated 74%)\n","  adding: hf-deployment/model/final_model.pkl (deflated 8%)\n","  adding: hf-deployment/model/word_embeddings.pt (deflated 8%)\n","  adding: hf-deployment/requirements.txt (stored 0%)\n","  adding: hf-deployment/.gradio/ (stored 0%)\n","  adding: hf-deployment/.gradio/certificate.pem (deflated 24%)\n","  adding: hf-deployment/app.py (deflated 67%)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_e7a45ed0-5e6a-48c2-a291-e673deab45bd\", \"hf-deployment.zip\", 296599184)"]},"metadata":{}}]}]}